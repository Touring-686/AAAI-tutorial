# Computational Optimization in LLM Inference: Reuse and Delegation
Large language models (LLMs) have recently demonstrated remarkable generation capabilities, achieving human-level or even superhuman performance on a wide range of downstream tasks. However, their substantial inference cost—driven by the inherently sequential nature of autoregressive decoding—has become a significant bottleneck for real-world deployment. This challenge has motivated the development of acceleration techniques based on computational optimization.
In this tutorial, we introduce a taxonomy that organizes existing approaches into two major families: **computation reuse** (e.g., key-value caching, prefix caching, parameter reuse), which reuses past computations to avoid redundancy, and**computation delegation**, which delegates the generation of draft tokens to auxiliary or intrinsic models, with verification performed by the target model. We cover the theoretical foundations, core algorithms, and practical implementations of these methods, concluding with their trade-offs, deployment challenges, and open research directions. This tutorial aims to equip both academic researchers and industry practitioners with the necessary knowledge to build more efficient LLM inference systems.
