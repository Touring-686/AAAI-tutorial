<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>Efficient LLM Inference: Computation Reuse and Delegation</title>

    <!-- favicon 
    <link href="favicon.png" rel=icon>-->

    <!-- web-fonts -->
    <link href='https://fonts.googleapis.com/css?family=Roboto:210,300,400,700,500' rel='stylesheet' type='text/css'>

    <!-- Bootstrap -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Style CSS -->
    <link href="css/style.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>
<body id="page-top" data-spy="scroll" data-target=".navbar">
<div id="main-wrapper">
    <!-- Page Preloader -->
    <div id="preloader">
        <div id="status">
            <div class="status-mes"></div>
        </div>
    </div>

    <header class="header">
        <!-- Navigation -->
        <nav class="navbar main-menu" role="navigation">
            <div class="container">
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                </div>

                <!-- Collect the nav links, forms, and other content for toggling -->
                <div class="collapse navbar-collapse navbar-right navbar-main-collapse">
                    <ul class="nav navbar-nav">
                        <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
                        <li class="hidden"><a href="#page-top"></a></li>
                        <li><a class="page-scroll" href="#section-intro">Abstract</a></li>
                        <li><a class="page-scroll" href="#section-background">Background</a></li>
                        <li><a class="page-scroll" href="#section-organizer">Core Paradigms</a></li>
                        <li><a class="page-scroll" href="#section-organizers">Presenters</a></li>
                    </ul>
                </div>
                <!-- /.navbar-collapse -->
            </div>
            <!-- /.container -->
        </nav>
    </header>

    <div class="jumbotron text-center">
        <div class="content">
            <h1>Efficient LLM Inference: Computation Reuse and Delegation</h1>
	    <h2 style="color:#FFFFFF;"><b>Keywords: LLM Inference Acceleration, Key-Value Caching, Speculative Decoding, Computational Optimization</b> </h2>
	    <h2 style="color:#FFFFFF;"><b>Event Date: 2025.1.21 </b> </h2>
        </div>
    </div>
    <!-- .Jumbotron-->
    
    
    <!-- Abstract Section -->
    <section id="section-intro" class="section-wrapper about-event">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h1>ABSTRACT</h1>
                </div>
            </div>
            <div class="row text-justify">
                <div class="col-md-12">
                   
		<p class="large text-muted">
		    Large language models (LLMs) have recently demonstrated remarkable generation capabilities, achieving human-level or even superhuman performance on a wide range of downstream tasks. However, their substantial inference cost—driven by the inherently sequential nature of autoregressive decoding—has become a significant bottleneck for real-world deployment. This challenge has motivated the development of acceleration techniques based on computational optimization.
		</p>
                    
		<p class="large text-muted">
			In this tutorial, we introduce a taxonomy that organizes existing approaches into two major families: <strong>computation reuse</strong> (e.g., key-value caching, prefix caching, parameter reuse), which reuses past computations to avoid redundancy, and <strong>computation delegation</strong>, which delegates the generation of draft tokens to auxiliary or intrinsic models, with verification performed by the target model. We cover the theoretical foundations, core algorithms, and practical implementations of these methods, concluding with their trade-offs, deployment challenges, and open research directions. This tutorial aims to equip both academic researchers and industry practitioners with the necessary knowledge to build more efficient LLM inference systems.
		</p>
		
		<p class="large text-muted">
		The topics of this tutorial include (but are not limited to) the following:
		<ol>	
			<strong> <li style="color:black;">Background and Motivation for LLM Inference Acceleration </li>  </strong>
			<strong> <li style="color:black;">Computation Reuse Paradigm (Key-Value Caching, Prefix Caching, Parameter Reuse) </li></strong>
			<strong> <li style="color:black;">Computation Delegation Paradigm (Speculative Decoding) </li></strong>
			<strong> <li style="color:black;">Collaborative Speculative Decoding Methods </li></strong>
			<strong> <li style="color:black;">Coupled Speculative Decoding Methods </li></strong>
			<strong> <li style="color:black;">Open Research Directions and Challenges </li></strong>
		</ol>
		</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Background Section -->
    <section id="section-background" class="section-wrapper about-event">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h1>BACKGROUND AND TOPIC</h1>
                </div>
            </div>
            <div class="row text-justify">
                <div class="col-md-12">
                   
		<p class="large text-muted">
		    Large language models (LLMs) have recently exhibited outstanding cross-task generalization and generation quality, achieving human-level or even superhuman performance on a range of downstream tasks, positioning them as a cornerstone technology in the pursuit of artificial general intelligence.
		</p>

		<p class="large text-muted">
		    However, while training benefits from large-scale parallelism, inference in LLMs remains inherently sequential. This autoregressive process requires executing the whole model at each decoding step, which leads to substantial latency and computational redundancy—especially in long-context scenarios—posing serious challenges for deployment at scale.
		</p>

		<p class="large text-muted">
		    To address these challenges, recent research has increasingly focused on computational optimization strategies, aiming to reduce redundant processing and accelerate generation without modifying model parameters or degrading output quality. These approaches fall broadly into two complementary paradigms: <strong>computation reuse</strong> and <strong>computation delegation</strong>. Computation reuse focuses on eliminating repeated work by reusing intermediate results from previous or similar decoding steps. This includes techniques such as key-value caching, prefix caching, which avoid recalculating hidden states or attention activation values when possible. In contrast, computation delegation, which is also known as speculative decoding, shifts part of the inference workload to auxiliary or intrinsic draft models, which generate candidate sequences that are selectively verified by the target model. This draft-then-verify pattern enables partial parallelism in the otherwise sequential decoding process. Together, these methods offer generality, compatibility with existing models, and substantial speedups. They have been widely adopted in modern LLM inference systems, including FasterTransformer, vLLM, and OpenAI's production stack.
		</p>

		<p class="large text-muted">
		    The key to computation reuse lies in eliminating redundant operations by leveraging previously computed information across decoding steps or between similar inputs. A widely adopted strategy is key-value (KV) caching, where attention keys and values from past tokens are stored and reused at each decoding step, avoiding recomputation of self-attention for the entire context. Recent works have further explored the reuse of information within a single session or across different sessions. By retrieving or matching similar prefixes from historical inputs, these methods leverage previously generated intermediate representations, such as hidden states, attention keys/values, or prompt structures, to reduce the computation cost of LLM inference. Moreover, reuse strategies have evolved from static linear prefix matching to structured reuse based on dynamic radix trees, enabling efficient management of complex multi-turn conversation history. Some works go further by retrieving past decoding results based on the input prefix. These results serve as references for current inference, and reducing computation overhead.
		</p>

		<p class="large text-muted">
		    For computation delegation, also known as speculative decoding, LLMs shift part of the generation workload from the large language model to more efficient draft generators. In this paradigm, lightweight auxiliary models first generate candidate sequences, and the target model then verifies them in parallel, thus reducing the number of full forward passes through the large model. Based on the relationship between the auxiliary draft model and the target model, these methods can be further categorized into two types: collaborative speculative decoding and coupled speculative decoding.
		</p>

		<p class="large text-muted">
		    In collaborative speculative decoding, the draft and target models operate as separate, loosely coupled systems. Drafting is typically handled by smaller, efficient models that propose full or partial candidate sequences, which are then verified in parallel by the larger target model. This paradigm allows for easy modular integration and is compatible with pre-trained models. In contrast, coupled speculative decoding establishes a tighter interaction between the draft and target models. The draft generation process is either embedded into the internal architecture of the target model or trained using intermediate representations extracted from it. This design enables more accurate drafts and higher verification efficiency, but often requires additional training and compromises modularity.
		</p>

		<p class="large text-muted">
		    In this tutorial, we provide a systematic overview of decoding-time computational optimization techniques in large language models, centered around two core paradigms: computation reuse, which eliminates redundant operations, and computation delegation, which reframes generation as a speculative draft-and-verify process. We will introduce representative methods, summarize recent research progress, and examine practical deployment challenges. By clarifying the design space and trade-offs of these strategies, this tutorial aims to equip both researchers and practitioners with the tools to build more efficient LLM-based systems.
		</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Core Paradigms Section -->
    <section id="section-organizer" class="section-wrapper team gray-bg">
        <div class="container">
            <div class="row">
                <div class="col-md-12">
                    <div class="section-title">
                        <h1>Core Paradigms: Computation Reuse and Computation Delegation</h1>
                    </div>
                </div>
            </div>
		
            <div class="row">
                    <figure class="image">
                        <a href="img/modeling_way.pdf"><img src="img/paradigm.png" class="img-responsive" alt="Image" style="text-align: center;"></a>
                        <figcaption class="caption text-center">
                            <!-- <h3>Paradigm Overview</h3> -->
                        </figcaption>
                    </figure>
            </div>
            <!-- .row -->
        </div>
        <!-- /.container -->
    </section>
    <!-- .team -->

    <!-- Organizers Section -->
    <section id="section-organizers" class="section-wrapper about-event">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h1>Presenters' Biography</h1>
                </div>
            </div>
            <div class="row text-justify">
                <div class="col-md-12">
                   
		<p class="large text-muted">
		    <strong>Zipeng Gao</strong> is a PH.D. student at the University of Science and Technology of China (USTC). He received his bachelor's degree from Dalian University of Technology in 2020 and began pursuing his Ph.D. at USTC in the same year. His research interests include speculative decoding, inference optimization for LLMs, and contextual content understanding. He has published papers in several top-tier international conferences and journals in the field of computer science, including AAAI, KDD, and ACM TALLIP.
		</p>

		<p class="large text-muted">
		    <strong>Dr. Zhi Zheng</strong> is a postdoc researcher at School of Computer Science and Technology, University of Science and Technology of China (USTC). He received his Ph.D. degree from USTC in 2024 and his B.E. degree from USTC in 2019. His research interests include Data Mining and Artificial Intelligence, especially on Large Language Models and Recommender Systems. He has published prolifically in refereed journals and conference proceedings, such as IEEE TKDE, ACM TOIS, SIGKDD, SIGIR, WWW, AAAI, IJCAI, CIKM and Recsys. He serves as top-tier conference program committee members (e.g., ICLR, NeurIPS, KDD, WWW, AAAI, IJCAI, SIGIR, etc.), and journal reviewers (e.g., TKDE, TKDD, TOIS, etc.). He received the COLING Outstanding Paper Award in 2025. More information about him can be found at <a href="https://zhengzhi-1997.github.io/" target="_blank">https://zhengzhi-1997.github.io/</a>.
		</p>

		<p class="large text-muted">
		    <strong>Dr. Qingrong Xia</strong> is a researcher at the AI System Innovation Lab, Huawei Cloud (China). He received his doctoral degree from Soochow University, China, in 2022. His research interests include syntactic/semantic parsing and pre-trained language models, with publications in top-tier conferences and journals such as AAAI, ACL, TACL, EMNLP, COLING, NAACL, etc. He actively contributes to the academic community as a reviewer, including AAAI, ACL, EMNLP, etc. He has won the best paper award in COLING 2022.
		</p>

		<p class="large text-muted">
		    <strong>Dr. Zhefeng Wang</strong> received the BE and PhD degrees in computer science from the University of Science and Technology of China, Hefei, in 2012 and 2017 respectively. He is currently working as a technical expert with Huawei Cloud, Hangzhou. His research interests include natural language processing, large language model, and AI system. He has published more than 50 papers in refereed conference proceedings and journals such as AAAI, ACL, ICML, and IEEE Transactions on Knowledge and Data Engineering.
		</p>

		<p class="large text-muted">
		    <strong>Dr. Baoxing Huai</strong>, Deputy Director of Huawei Cloud AI System Innovation Lab and Vice Chair of the Large Model and Generation Technical Committee (CIPS), holds a Ph.D. from USTC. His research spans AI systems, large models, NLP, machine learning, and data mining, with 40+ papers published at venues including AAAI, IJCAI, ACL, and TKDE. He brings deep AI product expertise, extensive algorithm optimization and B2B implementation experience, and 50+ invention patents.
		</p>

		<p class="large text-muted">
		    <strong>Dr. Tong Xu</strong> is currently working as a professor at University of Science and Technology of China (USTC), China. He has authored more than 100 top-tier journal and conference papers in related fields, including TKDE, TMC, TMM, TOMM, KDD, SIGIR, WWW, ACM MM, etc. He was the recipient of several paper awards of top conferences, including ACL and COLING, etc.
		</p>

		<p class="large text-muted">
		    <strong>Dr. Enhong Chen</strong> is a professor and vice dean of the School of Computer Science, USTC. His general area of research includes data mining, machine learning and social network analysis. He has published more than 300 papers in refereed conferences and journals, including Nature Communications, IEEE/ACM Transactions, KDD, NIPS, IJCAI and AAAI, etc. He received the Best Application Paper Award on KDD-2008, the Best Research Paper Award on ICDM-2011, and the Best of SDM-2015. His research is supported by the National Science Foundation for Distinguished Young Scholars of China.
		</p>
                </div>
            </div>
        </div>
    </section>

    <footer class="footer">
        <div class="copyright-section">
            <div class="container">
                <div class="col-md-12">
                    <div class="copytext text-center">Copyright &copy; 2024. All rights reserved.<a target="_blank" href="http://sc.chinaz.com/moban/">Webpage Template</a></div>
                </div>
            </div>
            <!-- .container -->
        </div>
        <!-- .copyright-section -->
    </footer>
    <!-- .footer -->

</div>
<!-- #main-wrapper -->


<!-- jquery -->
<script src="js/jquery-2.1.4.min.js"></script>

<!-- Bootstrap -->
<script src="js/bootstrap.min.js"></script>

<!-- Plugin JavaScript -->
<script src="js/jquery.easing.min.js"></script>

<!-- Google Maps API Key - Use your own API key to enable the map feature. More information on the Google Maps API can be found at https://developers.google.com/maps/ -->
<!---<script src="https://maps.googleapis.com/maps/api/js"></script>--->

<!--<script src="js/one-page-nav.js"></script>-->
<script src="js/scripts.js"></script>
</body>
</html>
